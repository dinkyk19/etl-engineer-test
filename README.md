## Overview ##

Rather than a FAANG style technical assessment on arcane algorithms, 
we hope to simulate a realistic scenario you might encounter as an ETL engineer at 
eXp Realty. The task involves importing and transforming data, and then creating a 
basic API to interact with the data.

Please read and follow the steps in this document carefully.  Feel free to follow up 
with any questions you may have. 

### Scenario ###

You are given the following: 
1. Three CSV files (member_feed.csv, office_feed.csv, and transaction_feed.csv) containing real estate related data.
2. A SQLite database named database.db with an existing schema (familiar from the prior SQL assessment) and populated 
with seed data. 

You're free to use whatever resources you can such as google or ChatGPT. But we can 
detect if code has been autogenerated and additionally, if the candidate moves on, there will 
be a panel round where we ask questions of your solution. Be prepared to discuss your decisons 
made during your development process. We'll look for things like readability, comments, and organization. Bonus points for 
any discussions or callouts about future performance enhancements or alternate cloud solutions. 

### Prerequistes ###
This project uses npm (Node Package Manager) for dependency management. 
Please ensure npm is installed and setup correctly:

1. Check if Node.js and npm are installed:
   ```bash
   node --version
   npm --version
   ```
   Both commands should return version numbers.

2. If Node.js/npm are not installed:
   - Windows: Download and run installer from https://nodejs.org/
   - Mac: Install via Homebrew:
     ```bash
     brew install node
     ```
   - Linux (Ubuntu/Debian):
     ```bash
     curl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash -
     sudo apt-get install -y nodejs
     ```

3. Verify installation:
   ```bash 
   npm --version
   ```
### Project Setup ### 
1. After cloning repository, navigate to project directory:
   ```bash
   cd etl-engineer-test
   ```

2. Install all required dependencies using npm:
   ```bash
   npm install
   ```

### Database Setup

1. Run the database setup script from the project root:
   ```bash
   ./scripts/run.sh
   ```
   Note: Make sure the script is executable by running:
   ```bash 
   chmod +x scripts/run.sh
   ```

This will:
- Create the database schema with 5 tables
- Load initial test data into the tables

### Verify Database Setup

1. Start an interactive SQLite session from the project root:
   ```bash
   sqlite3 sqlite/database.db
   ```

2. Verify tables are created correctly:
   ```sql
   sqlite> .tables
   ```
   You should see 5 tables listed: `expmember`, `member`, `office`, `transaction`, and `transactionreport`

3. Verify sample data was loaded:
   ```sql
   sqlite> select * from office;
   ```
   You should see 5 rows of office data. Feel free to explore the data in each table. 

4. Exit the SQLite session when finished:
   ```sql
   sqlite> .exit
   ```

## Assignment ## 
The following section details the project requirements. 

### Import, Transform, Save ###
Import and transform data from each CSV file into the corresponding tables found in the SQLite database. 
We recommend exploring the schema.sql and the interative SQLite session to determine where
fields from the CSV files should be saved.


There are 3 CSVs:
    office_feed.csv - contains all office related data. 
    member_feed.csv - contains all member data, including their office membership. 
    transaction_feed.csv - contains all transaction data as it relates to each member. 

Transform rules
1 - Ensure there are no duplicates for any records. 
2 - Any trailing or leading white space should be removed. 
3 - Phone numbers should be stored as strings and be of the same pattern. 
4 - Every record should have a create_at date and a modified_at date (datetime format).
5 - Monetary values should be displayed as dollars. 
6 - Boolean display values should true or false.
7 - Date/time values when returned via the API should be of the format YYYY-mm-dd 

We would like you to write in typescript a simple ETL pipeline that will ingest 
the 3 CSV files, apply the appropriate transformations, and then save to the 
database. We anticipate this as a number of files executed at the commandline that will
execute the pipeline.  

### API ### 

All of the responses below should be well formated JSON.  Please be aware
that not all of the imported fields may be used. 

We recommend using node express server as it is lightweight and suitable for APIs. We 
prefer either typescript for the solution. 

#### Office Endpoint #####

#### GET /office

Returns an array of objects for all offices with the following information:
- officekeynumeric
- Office Name 
- Office State/Province
- Office Status
- Created Date
- Modified Date

#### GET /office?status={status}

Returns the same office information as above, filtered by status.

Valid status values:
- `active` 
- `inactive`

#### Member Endpoint ####

#### GET /member?id={memberkeynumeric} ####
Return a member object with the following information:
    memberkeynumeric,
    membercity,
    memberemail,
    membersecondaryemail
    memberfirstname,
    memberfullname,
    memberlastname,
    membermiddlename,
    membermobilephone,
    memberstateorprovince,
    memberstatus,
    officekeynumeric,
    officename,
    anniversarydate,
    joindate,
    created at date
#### GET /member ####

Returns an array of the objects defined above for all members.

#### GET /member?office_id={officekeynumeric} ####

Returns an array of the objects defined above for all members for an individual office.


#### GET /transaction?transaction_id={transactionkeynumeric} #### 

transactionkeynumeric
transactionnumber
lifecyclestatus
salesprice
listprice
addressfull
stateorprovince
isreferral

#### GET /transaction?member_id={memberkeynumeric} #### 

Returns an array of transaction objects defined above for a given member. 

Bonus points for any test coverage you provide. 


When finished please send the compressed (zip archive) solution and send back to your technical contact at eXp Realty. 
